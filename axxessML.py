# -*- coding: utf-8 -*-
"""AXXESS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T-IrFJbKlmptSrKstA7YL0CeTECN1P9o
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder, StandardScaler
import tensorflow as tf
label_encoder = LabelEncoder()

data = pd.read_csv('COPD.csv')
dropper = ['Unnamed: 0', 'ID', 'FEV1PRED', 'FVCPRED', 'AGEquartiles', 'copd','MWT1Best','CAT']
data = data.drop(dropper, axis = 1)
data['smoking'] = data['smoking'] - 1
data['MWT1'] = (data['MWT1'] * 1.31)/6
data['MWT2'] = (data['MWT2'] * 1.31)/6
data['FEV1'] = (data['FEV1']* (10))
data['FVC'] = (data['FVC']* (10))
data['HAD'] = (data['HAD']/ (10))
data['SGRQ'] = (data['SGRQ']/ (10))

column_names = data.columns
data = data.fillna(0)
dataVal = data.values
for i in range(len(data.columns)):
  if isinstance(dataVal[0,i], str):
    print("Column: " + data.columns[i])
    data[data.columns[i]] = label_encoder.fit_transform(data[data.columns[i]])
    mapping_dict = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))
    print("Dictionary:")
    print(mapping_dict)

y = data['COPDSEVERITY']
x = data.drop('COPDSEVERITY', axis = 1 )

data

x

for i in range(4):
  print(sum(y==i))

new_y = y.values
new_x = x.values
from imblearn.over_sampling import RandomOverSampler

def normalizer(x, y, oversampler = False):
  scaler = StandardScaler()
  x = scaler.fit_transform(x)
  if oversampler:
    over = RandomOverSampler()
    x, y = over.fit_resample(x, y)

  data = np.hstack((x, np.reshape(y, (-1, 1))))

  return data, x, y
data, xdf, ydf = normalizer(new_x,new_y,oversampler=True)

for i in range(4):
  print(sum(ydf==i))

import random
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
import pickle

random.seed(42)

new_data = pd.DataFrame(data, columns=column_names)
new_data['COPDSEVERITY'] = pd.cut(new_data['COPDSEVERITY'], bins=4, labels=['Mild', 'Moderate', 'Severe', 'Very Severe'])

train, test = np.split(new_data.sample(frac=1), [int(len(new_data)*.7)])


x_train = train.drop("COPDSEVERITY", axis=1)
y_train = train['COPDSEVERITY']


x_test = test.drop("COPDSEVERITY", axis=1)
y_test = test["COPDSEVERITY"]

kNN = KNeighborsClassifier(n_neighbors=2)
kNN.fit(x_train, y_train)
with open('knn_model.pkl', 'wb') as f:
    pickle.dump(kNN, f)

for i in range(4):
  print(sum(ydf==i))

print(type(y_train))
print(type(x_train))

print(type(y_train))
print(type(x_train))

y_pred = kNN.predict(x_test)

print(classification_report(y_test, y_pred))

from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt


rf = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    min_samples_split=5,
    min_samples_leaf=2,
    max_features='sqrt',
    random_state=62,
    n_jobs=-1
)

rf.fit(x_train, y_train)

feature_importance = rf.feature_importances_

sorted_indices = feature_importance.argsort()[::-1]
sorted_features = [x_train.columns[i] for i in sorted_indices]

plt.figure(figsize=(10, 6))
plt.bar(range(x_train.shape[1]), feature_importance[sorted_indices])
plt.xticks(range(x_train.shape[1]), sorted_features, rotation=90)
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.title('Feature Importance')
plt.show()

def plot_loss(history):
  plt.plot(history.history['loss'], label = 'loss')
  plt.plot(history.history['val_loss'], label = 'val_loss'),
  plt.xlabel('Epoch')
  plt.ylabel("SparseCategoricalCrossentropy")
  plt.legend()
  plt.grid(True)
  plt.show()

def plot_accuracy(history):
  plt.plot(history.history['accuracy'], label = 'accuracy')
  plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
  plt.xlabel('Epoch')
  plt.ylabel("Accuracy")
  plt.legend()
  plt.grid(True)
  plt.show()

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import RandomOverSampler
from sklearn.model_selection import train_test_split


new_data = pd.DataFrame(data, columns=column_names)

new_data['COPDSEVERITY'] = pd.cut(new_data['COPDSEVERITY'], bins=4, labels=['Mild', 'Moderate', 'Severe', 'Very Severe'])

new_data = new_data.sample(frac=1, random_state=42)

x = new_data.drop("COPDSEVERITY", axis=1)
y = new_data['COPDSEVERITY']

label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

oversampler = RandomOverSampler(sampling_strategy='auto', random_state=42)
x_resampled, y_resampled = oversampler.fit_resample(x, y_encoded)

x_train, x_test, y_train, y_test = train_test_split(x_resampled, y_resampled, test_size=0.3, stratify=y_resampled, random_state=42)

model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(4, activation='softmax')
])

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
              metrics=['accuracy'])

history = model.fit(x_train, y_train, batch_size=32, epochs=400, validation_split=0.2, verbose=0)

y_pred = model.predict(x_test)
y_pred = np.argmax(y_pred, axis=1)

print(classification_report(y_test, y_pred))

plot_loss(history)
plot_accuracy(history)

model.save('./my_model.h5',save_format='h5')

y_test

print(pd.Series(y_test).value_counts())

y_pred = model.predict(x_test[:1])

print(np.argmax(y_pred, axis=1))

y_test[1]

x.shape

x.columns

# Age: Textbox (1-100)
# PackHistory: personâ€™s pack years smoking, where pack years is defined as twenty cigarettes smoked every day for one year (also open textbox)
# MWT1: Walk for 1 minute and count how many steps you can take (open textbox)
# MWT2: Walk for 1 minute and count how many steps you can take (open textbox)
# FEV1: Breath out for as long as you can and time it and enter in seconds (open textbox)
# FVC: DO IT AGAIN Breath out for as long as you can and time it and enter in seconds (open textbox)
# HAD: From 1-10 tell me how anxious and depressive you have been feeling (1-10 bar)
# SGRQ: From 1-10 tell me how your quality of life has been (1-10 bar)



len(x_train.columns)

tf.keras.models.save_model(model, '/content/drive', overwrite=True)

from google.colab import drive
drive.mount('/content/drive')

y_pred = model.predict(x_train[:7])

print(np.argmax(y_pred, axis=1))

x_train[:7]

x_train[1:2]

y_pred = model.predict(x_train[1:2])
print(np.argmax(y_pred, axis=1))



